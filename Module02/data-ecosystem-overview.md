# Module 2 – The Data Ecosystem: Overview & Key Concepts

##  Course & Module Context
- Course: *Introduction to Data Analytics* by IBM via Coursera  
- Module 2 covers the **Data Ecosystem**, including data types, formats, sources, tools, and repositories.

##  What I Learned

### 1. The Data Analyst Ecosystem
- Familiarized with the broad landscape of tools and frameworks used in modern data workflows - from collection to visualization.

### 2. Types of Data by Structure
| Data Type         | Description                                      |
|------------------|--------------------------------------------------|
| **Structured**     | Organized tables (e.g. SQL databases)             |
| **Semi-Structured** | Partly organized (e.g. JSON, XML, email)         |
| **Unstructured**   | No fixed format (e.g. video, audio, PDF)         |

### 3. Common File Formats
- **Delimited text** (e.g., CSV), **Excel**, **XML**, **JSON**, **PDF** — each with unique advantages and limitations for analytics.

### 4. Data Sources
- Data is sourced from:
  - Relational and non-relational **databases**
  - **APIs** and **web services**
  - Real-time **data streams**
  - **Social platforms**
  - **Sensors**

### 5. Languages Used by Data Professionals
- **Querying**: SQL for structured data retrieval  
- **Programming**: Python, R, Java for more flexible applications  
- **Shell/Scripting**: Unix/Linux Shell, PowerShell for automation

### 6. Data Repositories & Architecture
- **Database (RDBMS/NoSQL)** – structured vs semi-structured storage  
- **Data Warehouse** – centralized for analytics and reporting  
- **Data Mart** – focused subset of the warehouse for specific users  
- **Data Lake** – raw format store for all data types  
- **Big Data Stores** – distributed infrastructure for handling massive volumes

### 7. ETL & Data Pipelines
- **ETL (Extract, Transform, Load):**
  1. Extract from sources  
  2. Transform via cleansing and normalization  
  3. Load into repositories
- **Data Pipeline** = end-to-end flow leveraging ETL practices

### 8. Big Data & Supporting Tools
- Big Data characterized by **volume**, **velocity**, and **variety**
- Scale challenges met using:
  - **Hadoop (and HDFS)**  
  - **Hive** (SQL-like queries on Hadoop)  
  - **Spark** (fast, in-memory analytics engine)

##  Suggested Repository Folder
`data-analytics-journey/module-2-data-ecosystem`

##  Reflection
This module built a solid foundation of how a data analyst interacts with diverse data types, repositories, and tools — showing the full journey from raw data to actionable insight.


